{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot segmentation notebook \n",
    "# Elizabeth Finn\n",
    "# May 24, 2023\n",
    "\n",
    "## Will:\n",
    "\n",
    "1. Find nuclei from nuclear segmentation results\n",
    "2. Load 10 at random according to your wells of interest\n",
    "3. Allow iterative optimization of parameters\n",
    "4. Segment spots using a Laplacian of the Gaussian for selected nuclei and parameters\n",
    "5. Output to a config file for bulk analysis\n",
    "6. Format, write, and run shell scripts for bulk analysis\n",
    "\n",
    "# First, set basic parameters and load in necessary tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ANALYSIS_PREFIX = \"2023-07-18\"\n",
    "MICROSCOPE = \"IMX\"\n",
    "USER = \"butlerm\"\n",
    "DAPI_CHANNEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set filetype environment variable here, before anything is loaded... \"LSM\" is LSM, \"IMX\" is default and for ImageXpress Micro\n",
    "import os\n",
    "os.environ[\"FILE_TYPE\"] = MICROSCOPE\n",
    "os.environ[\"USER\"] = USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import some tools that we will use:\n",
    "# Utilities\n",
    "import json\n",
    "import random\n",
    "import numpy\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "#Sci-kit image and matplotlib\n",
    "import skimage.io\n",
    "import skimage.measure\n",
    "import skimage.feature\n",
    "import skimage.segmentation\n",
    "from skimage.morphology import disk\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Our bespoke tools\n",
    "from models.swarm_job import SwarmJob, RunStrategy\n",
    "from models.image_filename_glob import ImageFilenameGlob\n",
    "from models.generate_spot_positions_config import GenerateSpotPositionsConfig\n",
    "from generate_spot_positions import GenerateSpotPositionsJob\n",
    "from generate_all_spot_positions import GenerateAllSpotPositionsJob\n",
    "from generate_all_spot_result_lines import GenerateAllSpotResultLinesJob\n",
    "from generate_spot_results_file import GenerateSpotResultsFileJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM HERE, YOU MUST RUN ITERATIVELY, MULTIPLE TIMES PER CHANNEL, AS YOU OPTIMIZE. \n",
    "\n",
    "## The overall process looks like this:\n",
    "### For each channel:\n",
    "#### Do the following until selecting a new set of random nuclei doesn't substantially alter the segmentation (i.e. until it looks good every time you re-cycle through positions and samples). Be sure to test at least five different positions (wells or coverslips) and at least three different random subsets of nuclei per position:\n",
    "        1. Select a random set of 10 nuclei (first chunk)\n",
    "        2. Choose parameters (second chunk)\n",
    "        3. Plot nuclei and spot results (third chunk)\n",
    "#### Once that is done, and you're comfortable with the parameters for the channel of interest, add that channel's parameters to the config file (fourth chunk). Note that you don't need to change anything, it will automatically detect if there's already a config file and append/replace appropriately if so.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_folder = \"/s/finn-lab/MB_0718/crops/\"\n",
    "image_path = Path(image_folder)\n",
    "\n",
    "position_ID = \"C07\"\n",
    "channel_ID = 2\n",
    "\n",
    "image_filename_glob = image_path.rglob(str(ImageFilenameGlob(position = position_ID,\n",
    "                                                             c = channel_ID, \n",
    "                                                             suffix=\"_maximum_projection_nucleus_???\", \n",
    "                                                             extension=\"npy\")))\n",
    "\n",
    "cells_list = ([str(image_file_path) for image_file_path in image_filename_glob])\n",
    "files_count = len(cells_list)\n",
    "if files_count == 0:\n",
    "    print(\"I didn't find any nuclei, check another field.\")\n",
    "else: \n",
    "    images_selected = random.sample(cells_list, min(10, files_count))\n",
    "    if files_count < 10:\n",
    "        print(\"I only found \", files_count,  \"nuclei\")\n",
    "\n",
    "for image_selected in images_selected:\n",
    "    print(image_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the parameters and display nuclei with spots highlighted in the next two chunks. With each batch of 10 nuclei, run the next two chunks as many times as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Threshold is applied to the laplacian of the gaussian, it is fairly cryptic, so trial and error works best to find a good value.\n",
    "LoG_threshold = 0.8\n",
    "\n",
    "# Fold change over local (radius *2) average\n",
    "local_contrast_threshold = 1.2\n",
    "\n",
    "# Peak radius is applied to pre-filter the spots; anything smaller than this will get filtered out, so err on the small side. \n",
    "# It ought to be near the actual radius of the visible spot in pixels.\n",
    "peak_radius = 2.0\n",
    "\n",
    "# Because images are normalized to 0-1 range in the cell, your spot maxima are set to 1. So this ought to be the inverse of your overall signal:noise ratio\n",
    "# Counterintuitively, better FISH -> lower threshold here -> less filtering taking place on the actual image\n",
    "global_contrast_threshold = 0.3\n",
    "\n",
    "segmenters = [GenerateSpotPositionsJob(image, \"no_output\", image_folder, LoG_threshold, local_contrast_threshold, peak_radius, global_contrast_threshold) for image in images_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5)\n",
    "\n",
    "for i in range(0,10):\n",
    "    ax = axs[i//5, i%5]\n",
    "    ax.imshow(segmenters[i].image)\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.get_yaxis().set_visible(True)\n",
    "    for spot in segmenters[i].spots:\n",
    "        circle=plt.Circle((spot[4], spot[3]), 7, color='r', alpha=0.3)\n",
    "        ax.add_patch(circle)\n",
    "        #ax.text(x = spot[1]-20, y = spot[0]-15, s = \"%.2f, %.2f\"%(spot[1], spot[0]), bbox=dict(fill=True))\n",
    "\n",
    "fig.set_size_inches(15,7)\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the selected parameters into a config file. This chunk can be run multiple times per channel but must be run at least once per channel -- if you have channels without parameters, they will not be segmented. Code will:\n",
    "1. If a config file exists, open it and read it into a table.\n",
    "2. Alter that table as necessary, or generate a new one with the current channel and parameters.\n",
    "3. Overwrite the config file (or write a new config file) with the updated table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_file = \"/hpc-prj/finn/Results/configs/MBspotsconfig.json\"\n",
    "\n",
    "config_file_path = Path(config_file)\n",
    "\n",
    "existing_generate_spot_positions_configs = []\n",
    "if config_file_path.exists():\n",
    "    with open(config_file_path) as json_file:\n",
    "        existing_generate_spot_positions_configs_json_params = json.load(json_file)\n",
    "        existing_generate_spot_positions_configs = [\n",
    "            GenerateSpotPositionsConfig.from_json_params(json_params)\n",
    "            for json_params\n",
    "            in existing_generate_spot_positions_configs_json_params\n",
    "        ]\n",
    "\n",
    "current_generate_spot_positions_config = GenerateSpotPositionsConfig(\n",
    "    channel=channel_ID,\n",
    "    LoG_threshold=LoG_threshold,\n",
    "    local_contrast_threshold=local_contrast_threshold,\n",
    "    peak_radius=peak_radius,\n",
    "    global_contrast_threshold=global_contrast_threshold\n",
    ")\n",
    "\n",
    "updated_generate_spot_positions_configs = [\n",
    "    current_generate_spot_positions_config,\n",
    "    *(\n",
    "        existing_generate_spot_positions_config\n",
    "        for existing_generate_spot_positions_config\n",
    "        in existing_generate_spot_positions_configs\n",
    "        if existing_generate_spot_positions_config.channel != current_generate_spot_positions_config.channel\n",
    "    )\n",
    "]\n",
    "\n",
    "updated_generate_spot_positions_configs_json_params = [config.to_json_params() for config in updated_generate_spot_positions_configs]\n",
    "with open(config_file_path, 'w') as write_file:\n",
    "    json.dump(updated_generate_spot_positions_configs_json_params, write_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're ready to run in batch mode! Congratulations! \n",
    "### Here's what the following chunks do:\n",
    "1. Load in relevant parameters and folders. (Parameters: local or parallel processing via RunStrategy and location of config file.)\n",
    "2. Segment all the spots\n",
    "3. Determine centers of gravity, size, shape, and intensity for all spots.\n",
    "4. Collate everything into a csv file.\n",
    "\n",
    "### Each chunk will generate a shell script, run it, and email you approximately when it is completed. However, it is generally faster and more reliable to monitor the shell script using the \"squeue\" command on the command line. Do not run chunk two until the shell script in chunk one has finished running, since the files will not be all generated and will therefore get lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find your files!\n",
    "SwarmJob.run_strategy = RunStrategy.SWARM\n",
    "\n",
    "source_cropped_images = \"/s/finn-lab/MB_0718/crops/\"\n",
    "source_nuclear_masks = \"/s/finn-lab/MB_0718/masks/\"\n",
    "source_distance_transforms = \"/s/finn-lab/MB_0718/dts/\"\n",
    "\n",
    "results_directory = Path(\"/hpc-prj/finn/Results\")\n",
    "scratch_directory = Path(\"/s/finn-lab/MB_0718\")\n",
    "spot_positions_directory = scratch_directory / \"spot_positions/\"\n",
    "spot_result_lines_directory = scratch_directory / \"spot_result_lines/\"\n",
    "\n",
    "log_directory = results_directory / \"logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the spot segmentation! \n",
    "\n",
    "GenerateAllSpotPositionsJob(source_cropped_images, spot_positions_directory, log_directory, DAPI_CHANNEL, config_file).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating file dictionary (this can take a while)\n",
      "running generate_all_spot_result_lines_20230721143357 in parallel mode\n",
      "['sbatch', '--wait', PosixPath('/s/finn-lab/MB_0718/spot_result_lines/generate_all_spot_result_lines_20230721143357.sh')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1911659\n"
     ]
    }
   ],
   "source": [
    "# From spot segmentations, find x, y, z, and r positions!\n",
    "GenerateAllSpotResultLinesJob(spot_positions_directory, source_cropped_images, \n",
    "                              source_distance_transforms, source_nuclear_masks, \n",
    "                              spot_result_lines_directory, log_directory).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find and merge all spot positions into a single file!\n",
    "GenerateSpotResultsFileJob(spot_result_lines_directory, results_directory, ANALYSIS_PREFIX).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
